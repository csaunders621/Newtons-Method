{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "118c9723",
   "metadata": {},
   "source": [
    "# Logistic Regression with Newton's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d72866d",
   "metadata": {},
   "source": [
    "Objective: create logistic regression model from scratch using Newton's method to optimize weights to minimize cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1574311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691df401",
   "metadata": {},
   "source": [
    "First we will create functions to compute the log likelihood, gradient, and hessian, as well as the logistic regression train and predict functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa01a768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglikelihood(p, y):\n",
    "    l = np.sum(y.dot(np.log(p)) + (1-y).dot(np.log(1-p)))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e02b21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(p, x, y):\n",
    "    y = np.expand_dims(y, axis=-1)\n",
    "    grad = np.array([[np.sum((y - p) * x), np.sum(y - p)]])\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baa431a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian(p, x, y):\n",
    "    H = np.array([[np.sum(p * (1 - p) * x * x), np.sum(p * (1 - p) * x)], [np.sum(p * (1 - p) * x), np.sum(p * (1 - p))]])\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56ed693b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_reg_train(X, y):\n",
    "    \n",
    "    # initialize weights    \n",
    "    w0 = 0\n",
    "    w1 = np.ones((X.shape[1], 1))\n",
    "\n",
    "    # calculate sigmoid probability and log likelihood\n",
    "    a = X.dot(w1) + w0\n",
    "    p = 1 / (1 + np.exp((-a).astype(float)))\n",
    "    l = loglikelihood(p, y)\n",
    "    \n",
    "    delta = 100\n",
    "    while delta > 0.000001:\n",
    "        # calculate gradient and hessian\n",
    "        g = gradient(p, X, y)\n",
    "        H = hessian(p, X, y)\n",
    "        \n",
    "        # compute our step\n",
    "        w_new = (np.linalg.inv(H)).dot(g.T)\n",
    "        \n",
    "        # update weights\n",
    "        w0 += w_new[1]\n",
    "        w1 += w_new[0]\n",
    "\n",
    "        # compute new sigmoid probability and log likelihood with new weights\n",
    "        a_new = X.dot(w1) + w0\n",
    "        p_new = 1 / (1 + np.exp((-a_new).astype(float)))\n",
    "        l_new = loglikelihood(p, y)\n",
    "        delta = abs(l - l_new)\n",
    "        l = l_new\n",
    "        \n",
    "    return (w0, w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19303499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_reg_predict(w0, w1, X, y):\n",
    "    \n",
    "    # compute sigmoid probability\n",
    "    a = X_test.dot(w1)  + w0\n",
    "    sig = 1 / (1 + np.exp((-a).astype(float)))\n",
    "    \n",
    "    # record labels as 0 if the probability is less than 0.5 and 1 otherwise\n",
    "    y_pred = np.array([0 if s < 0.5 else 1 for s in sig])\n",
    "    \n",
    "    # calculate accuracy between computed labels and ground truth labels\n",
    "    correct = 0\n",
    "    for i in range(len(y)):\n",
    "        if y_pred[i] == y[i]:\n",
    "            correct += 1\n",
    "            \n",
    "    acc = correct / len(y)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d3f2d2",
   "metadata": {},
   "source": [
    "Next we will prepare and clean the dataset. Column 7 of the dataset contains 16 tuples with a missing value, which has been replaced with the character '?'. Because of this, each entry of column 7 is stored as a string instead of an integer. To remedy this, we will calculate the mean of that column and replace each missing value with the mean, as well as convert each existing value to an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7fcba30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('breast-cancer-wisconsin.data', header = None)\n",
    "data = np.array(df)\n",
    "\n",
    "# First calculate the mean of the column\n",
    "col = data[:,6]\n",
    "cnt = 0\n",
    "idx = 0\n",
    "missing = 0\n",
    "while idx < (len(col)-1):\n",
    "    if col[idx] == '?':\n",
    "        idx += 1\n",
    "        missing += 1\n",
    "    else:\n",
    "        cnt += int(col[idx])\n",
    "        idx += 1\n",
    "mean = cnt // (len(col) - missing)\n",
    "        \n",
    "# If the value is missing, replace it with the mean, otherwise convert the value to an integer\n",
    "for x in range(col.shape[0]):\n",
    "    if data[x][6] == '?':\n",
    "        data[x][6] = mean\n",
    "    else:\n",
    "        data[x][6] = int(data[x][6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821ffbd1",
   "metadata": {},
   "source": [
    "The class labels in this dataset are stored as a 2 for benign and 4 for malignant, so now we will split the data between X (data) and y (labels) and convert those labels to 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcc59c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(699, 9) (699,)\n"
     ]
    }
   ],
   "source": [
    "# take columns 1-9 as our data X, and our last column as the labels y.\n",
    "X = data[:, 1:-1]\n",
    "y = data[:, 10]\n",
    "\n",
    "# convert labels from 2 and 4 to 0 and 1\n",
    "for i in range(len(y)):\n",
    "    if y[i] == 2:\n",
    "        y[i] = 0\n",
    "    else:\n",
    "        y[i] = 1\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2f4b40",
   "metadata": {},
   "source": [
    "Now we are ready to train and predict with the logistic regression model. We want to train and predict ten times, each time with a new random split of data between the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3fbd3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies:  [0.9785714285714285, 0.9642857142857143, 0.9785714285714285, 0.9785714285714285, 0.9785714285714285, 0.9785714285714285, 0.95, 0.9714285714285714, 0.9571428571428572, 0.9714285714285714]\n",
      "Average Accuracy: 0.9707142857142858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carme\\AppData\\Local\\Temp\\ipykernel_24924\\25609289.py:2: RuntimeWarning: divide by zero encountered in log\n",
      "  l = np.sum(y.dot(np.log(p)) + (1-y).dot(np.log(1-p)))\n",
      "C:\\Users\\carme\\AppData\\Local\\Temp\\ipykernel_24924\\1009655005.py:27: RuntimeWarning: overflow encountered in exp\n",
      "  p_new = 1 / (1 + np.exp((-a_new).astype(float)))\n",
      "C:\\Users\\carme\\AppData\\Local\\Temp\\ipykernel_24924\\3241350734.py:5: RuntimeWarning: overflow encountered in exp\n",
      "  sig = 1 / (1 + np.exp((-a).astype(float)))\n"
     ]
    }
   ],
   "source": [
    "# array to store accuracies\n",
    "accs = []\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    # shuffle the data\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    X = X[indices]\n",
    "    y = y[indices]\n",
    "    \n",
    "    # create 80-20 split between training set and test set \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "    \n",
    "    # train, predict, and store the accuracy\n",
    "    w0, w1 = logistic_reg_train(X_train, y_train)\n",
    "    acc = logistic_reg_predict(w0, w1, X_test, y_test)\n",
    "    accs.append(acc)\n",
    "    \n",
    "print('Accuracies: ' , accs)\n",
    "print('Average Accuracy:', np.mean(accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bfbca4",
   "metadata": {},
   "source": [
    "We can use the SciKit-Learn Logistic Regression model and compare our accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed6e295d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9714285714285714\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "model = LogisticRegression().fit(X_train.astype('int'), y_train.astype('int'))\n",
    "scores = model.predict(X_test.astype('int'))\n",
    "\n",
    "print('Accuracy: ' , accuracy_score(y_test.astype('int'), scores.astype('int')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffad6d0",
   "metadata": {},
   "source": [
    "Let's try this same process on normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1b4658c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies:  [0.6, 0.6357142857142857, 0.6571428571428571, 0.6714285714285714, 0.6428571428571429, 0.6714285714285714, 0.6071428571428571, 0.7428571428571429, 0.6428571428571429, 0.6714285714285714]\n",
      "Average Accuracy: 0.6542857142857144\n"
     ]
    }
   ],
   "source": [
    "# array to store accuracies\n",
    "accs = []\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    # shuffle the data\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    X = X[indices]\n",
    "    y = y[indices]\n",
    "    \n",
    "    # create 80-20 split between training set and test set \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "    X_train = preprocessing.normalize(X_train, axis=0)\n",
    "    X_test = preprocessing.normalize(X_test, axis=0)\n",
    "    \n",
    "    # train, predict, and store the accuracy\n",
    "    w0, w1 = logistic_reg_train(X_train, y_train)\n",
    "    acc = logistic_reg_predict(w0, w1, X_test, y_test)\n",
    "    accs.append(acc)\n",
    "    \n",
    "print('Accuracies: ' , accs)\n",
    "print('Average Accuracy:', np.mean(accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baa2f010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6142857142857143\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "X_train = preprocessing.normalize(X_train, axis=0)\n",
    "X_test = preprocessing.normalize(X_test, axis=0)\n",
    "model = LogisticRegression().fit(X_train.astype('int'), y_train.astype('int'))\n",
    "scores = model.predict(X_test.astype('int'))\n",
    "\n",
    "print('Accuracy: ' , accuracy_score(y_test.astype('int'), scores.astype('int')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d4a163",
   "metadata": {},
   "source": [
    "With the normalized data we do not get a Runtime Warning, but we also get a much lower accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
